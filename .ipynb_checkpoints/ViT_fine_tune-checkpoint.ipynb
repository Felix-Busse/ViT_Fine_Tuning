{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73771b80-f5ad-4514-b451-62f744d0f62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 14:18:00.282739: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-12 14:18:00.379759: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-12 14:18:00.379793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-12 14:18:00.388823: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-12 14:18:00.416860: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-12 14:18:00.417665: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-12 14:18:01.886253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import imageio.v2 as iio\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks\n",
    "from transformers import ViTImageProcessor, TFViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d27b526-0c84-4add-a3c8-17d695d8f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_vi_t_for_image_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  769000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86567656 (330.23 MB)\n",
      "Trainable params: 86567656 (330.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ViTImageProcessor and use ViT base model as pretrained model\n",
    "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "# Create an instance of ViTForImageClassification and use ViT base model as pretrained model\n",
    "model = TFViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29457a2f-147e-4a21-bc3a-06ba7baa3c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def load_image_data(path):\n",
    "    '''Function will load image data and process it with tensorflow decode_image function.\n",
    "    Parameters:\n",
    "    path <str>: file path string to load the image from\n",
    "\n",
    "    Return:\n",
    "    <tf.Tensor>: Decoded image with shape (height, width, channels)\n",
    "    '''\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.io.decode_image(image, channels=3)\n",
    "    image.set_shape([3, 224, 224])\n",
    "    return image\n",
    "\n",
    "@tf.py_function(Tout=tf.float32)\n",
    "def apply_feature_extractor(x):\n",
    "    '''Function will apply \"feature_extractor\" to image data. Decorator allows usage of this function inside tf.data.Dataset.map().\n",
    "    Parameters:\n",
    "    x <tf.Tensor>: image data\n",
    "\n",
    "    Return:\n",
    "    <tf.Tensor>: features of image data\n",
    "    '''\n",
    "    image = feature_extractor(images=x, return_tensors='tf')['pixel_values']\n",
    "    print(image.shape)\n",
    "    image.set_shape([3, 224, 224])\n",
    "    return image\n",
    "\n",
    "# Create tf.data.Dataset containing all \".jpg\" file names in folder\n",
    "image_pipeline = tf.data.Dataset.list_files('*.jpg')\n",
    "# Load the image date from file names\n",
    "image_pipeline = image_pipeline.map(lambda x: load_image_data(x))\n",
    "# Batch Dataset\n",
    "image_pipeline = image_pipeline.batch(1)\n",
    "# Apply feature_extractor to all image data\n",
    "\n",
    "image_pipeline = image_pipeline.map(lambda x: apply_feature_extractor(x))\n",
    "image_pipeline\n",
    "# Adding labels \n",
    "#image_pipeline = image_pipeline.map(lambda x: (x, 'cat'))\n",
    "#image_pipeline = image_pipeline.unbatch()\n",
    "#image_pipeline = image_pipeline.batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b174be-2884-48c3-9e26-5bee0b3d81a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 224, 224)\n",
      "(1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 14:18:20.922957: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: ValueError: Tensor's shape (1, 3, 224, 224) is not compatible with supplied shape [3, 224, 224].\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "    return func(device, token, args)\n",
      "\n",
      "  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n",
      "    outputs = self._call(device, args)\n",
      "\n",
      "  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n",
      "    ret = self._func(*args)\n",
      "\n",
      "  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/tmp/ipykernel_11442/1434880300.py\", line 25, in apply_feature_extractor\n",
      "    image.set_shape([3, 224, 224])\n",
      "\n",
      "  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 534, in set_shape\n",
      "    raise ValueError(f\"Tensor's shape {self.shape} is not compatible \"\n",
      "\n",
      "ValueError: Tensor's shape (1, 3, 224, 224) is not compatible with supplied shape [3, 224, 224].\n",
      "\n",
      "\n",
      "2024-07-12 14:18:20.958183: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: ValueError: Tensor's shape (1, 3, 224, 224) is not compatible with supplied shape [3, 224, 224].\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "    return func(device, token, args)\n",
      "\n",
      "  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n",
      "    outputs = self._call(device, args)\n",
      "\n",
      "  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n",
      "    ret = self._func(*args)\n",
      "\n",
      "  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/tmp/ipykernel_11442/1434880300.py\", line 25, in apply_feature_extractor\n",
      "    image.set_shape([3, 224, 224])\n",
      "\n",
      "  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 534, in set_shape\n",
      "    raise ValueError(f\"Tensor's shape {self.shape} is not compatible \"\n",
      "\n",
      "ValueError: Tensor's shape (1, 3, 224, 224) is not compatible with supplied shape [3, 224, 224].\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: Tensor's shape (1, 3, 224, 224) is not compatible with supplied shape [3, 224, 224].\nTraceback (most recent call last):\n\n  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n    return func(device, token, args)\n\n  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n    outputs = self._call(device, args)\n\n  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n    ret = self._func(*args)\n\n  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/tmp/ipykernel_11442/1434880300.py\", line 25, in apply_feature_extractor\n    image.set_shape([3, 224, 224])\n\n  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 534, in set_shape\n    raise ValueError(f\"Tensor's shape {self.shape} is not compatible \"\n\nValueError: Tensor's shape (1, 3, 224, 224) is not compatible with supplied shape [3, 224, 224].\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_pipeline):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#model.predict(val)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(val))\n\u001b[1;32m      4\u001b[0m image_pipeline\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3029\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3027\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3029\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3031\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: Tensor's shape (1, 3, 224, 224) is not compatible with supplied shape [3, 224, 224].\nTraceback (most recent call last):\n\n  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n    return func(device, token, args)\n\n  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n    outputs = self._call(device, args)\n\n  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n    ret = self._func(*args)\n\n  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/tmp/ipykernel_11442/1434880300.py\", line 25, in apply_feature_extractor\n    image.set_shape([3, 224, 224])\n\n  File \"/home/felbus/transformers/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 534, in set_shape\n    raise ValueError(f\"Tensor's shape {self.shape} is not compatible \"\n\nValueError: Tensor's shape (1, 3, 224, 224) is not compatible with supplied shape [3, 224, 224].\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(image_pipeline):\n",
    "    #model.predict(val)\n",
    "    print(type(val))\n",
    "image_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "25018d33-b179-4dc1-aa59-de1b0c5b7f66",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2436, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2409, in run_step  *\n        outputs = model.predict_step(data)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2377, in predict_step  *\n        return self(x, training=False)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__  *\n        return super().__call__(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n        raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n    File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n        retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification).\n    \n    in user code:\n    \n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 872, in call  *\n            outputs = self.vit(\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n            return fn(*args, **kwargs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n            outputs = call_fn(inputs, *args, **kwargs)\n        File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n            raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n        File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n            retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n    \n        OperatorNotAllowedInGraphError: Exception encountered when calling layer 'vit' (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 596, in call  *\n                embedding_output = self.embeddings(\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n                return fn(*args, **kwargs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n                outputs = call_fn(inputs, *args, **kwargs)\n            File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n                raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n            File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n                retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n        \n            OperatorNotAllowedInGraphError: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 129, in call  *\n                    batch_size, num_channels, height, width = shape_list(pixel_values)\n            \n                OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n            \n            \n            Call arguments received by layer 'embeddings' (type TFViTEmbeddings):\n              • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=False\n        \n        \n        Call arguments received by layer 'vit' (type TFViTMainLayer):\n          • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n          • head_mask=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification):\n      • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • labels=None\n      • training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_pipeline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1249\u001b[0m, in \u001b[0;36mTFPreTrainedModel.predict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mpredict)\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1248\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2436, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2409, in run_step  *\n        outputs = model.predict_step(data)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2377, in predict_step  *\n        return self(x, training=False)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__  *\n        return super().__call__(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n        raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n    File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n        retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification).\n    \n    in user code:\n    \n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 872, in call  *\n            outputs = self.vit(\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n            return fn(*args, **kwargs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n            outputs = call_fn(inputs, *args, **kwargs)\n        File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n            raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n        File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n            retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n    \n        OperatorNotAllowedInGraphError: Exception encountered when calling layer 'vit' (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 596, in call  *\n                embedding_output = self.embeddings(\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n                return fn(*args, **kwargs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n                outputs = call_fn(inputs, *args, **kwargs)\n            File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n                raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n            File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n                retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n        \n            OperatorNotAllowedInGraphError: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 129, in call  *\n                    batch_size, num_channels, height, width = shape_list(pixel_values)\n            \n                OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n            \n            \n            Call arguments received by layer 'embeddings' (type TFViTEmbeddings):\n              • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=False\n        \n        \n        Call arguments received by layer 'vit' (type TFViTMainLayer):\n          • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n          • head_mask=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification):\n      • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • labels=None\n      • training=False\n"
     ]
    }
   ],
   "source": [
    "model.predict(image_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "921d7aa0-3fc0-4ba0-9173-1798e9b1d429",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2436, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2409, in run_step  *\n        outputs = model.predict_step(data)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2377, in predict_step  *\n        return self(x, training=False)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__  *\n        return super().__call__(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n        raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n    File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n        retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification).\n    \n    in user code:\n    \n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 872, in call  *\n            outputs = self.vit(\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n            return fn(*args, **kwargs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n            outputs = call_fn(inputs, *args, **kwargs)\n        File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n            raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n        File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n            retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n    \n        OperatorNotAllowedInGraphError: Exception encountered when calling layer 'vit' (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 596, in call  *\n                embedding_output = self.embeddings(\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n                return fn(*args, **kwargs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n                outputs = call_fn(inputs, *args, **kwargs)\n            File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n                raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n            File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n                retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n        \n            OperatorNotAllowedInGraphError: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 129, in call  *\n                    batch_size, num_channels, height, width = shape_list(pixel_values)\n            \n                OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n            \n            \n            Call arguments received by layer 'embeddings' (type TFViTEmbeddings):\n              • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=False\n        \n        \n        Call arguments received by layer 'vit' (type TFViTMainLayer):\n          • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n          • head_mask=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification):\n      • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • labels=None\n      • training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(val)\n\u001b[1;32m      5\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m pipeline\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1249\u001b[0m, in \u001b[0;36mTFPreTrainedModel.predict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mpredict)\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1248\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2436, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2409, in run_step  *\n        outputs = model.predict_step(data)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2377, in predict_step  *\n        return self(x, training=False)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__  *\n        return super().__call__(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n        raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n    File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n        retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification).\n    \n    in user code:\n    \n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 872, in call  *\n            outputs = self.vit(\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n            return fn(*args, **kwargs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n            outputs = call_fn(inputs, *args, **kwargs)\n        File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n            raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n        File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n            retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n    \n        OperatorNotAllowedInGraphError: Exception encountered when calling layer 'vit' (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 596, in call  *\n                embedding_output = self.embeddings(\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n                return fn(*args, **kwargs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n                outputs = call_fn(inputs, *args, **kwargs)\n            File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 162, in error_handler  **\n                raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n            File \"/tmp/__autograph_generated_filew8h6qunk.py\", line 34, in error_handler\n                retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n        \n            OperatorNotAllowedInGraphError: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 129, in call  *\n                    batch_size, num_channels, height, width = shape_list(pixel_values)\n            \n                OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n            \n            \n            Call arguments received by layer 'embeddings' (type TFViTEmbeddings):\n              • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=False\n        \n        \n        Call arguments received by layer 'vit' (type TFViTMainLayer):\n          • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n          • head_mask=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification):\n      • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • labels=None\n      • training=False\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = tf.constant([1])\n",
    "for i, val in enumerate(image_pipeline):\n",
    "    tensor_1 = val\n",
    "pipeline = tf.data.Dataset.from_tensor_slices(val)\n",
    "pipeline = pipeline.batch(1)\n",
    "model.predict(pipeline)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4b7557a-c142-4410-930d-54f6bf1cc2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.data.ops.options.Options at 0x7f5106db8280>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ToDo : Learn about shapes and that shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29349788-e1f2-45cb-9b11-de660b9df92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 492ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic = tf.io.read_file('sample.jpg')\n",
    "pic = tf.io.decode_image(pic)\n",
    "pic_features = feature_extractor(pic, return_tensors='tf')\n",
    "prediction = model.predict(pic_features['pixel_values'])\n",
    "pic_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7219e909-ce28-4458-a768-d98190f46502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
