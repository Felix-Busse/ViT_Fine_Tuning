{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73771b80-f5ad-4514-b451-62f744d0f62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 20:54:52.080921: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-21 20:54:52.153050: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-21 20:54:52.153085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-21 20:54:52.156973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-21 20:54:52.170989: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-21 20:54:52.172926: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 20:54:53.679384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import imageio.v2 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks\n",
    "from transformers import ViTImageProcessor, TFViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d27b526-0c84-4add-a3c8-17d695d8f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_vi_t_for_image_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  769000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86567656 (330.23 MB)\n",
      "Trainable params: 86567656 (330.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ViTImageProcessor and use ViT base model as pretrained model\n",
    "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "# Create an instance of ViTForImageClassification and use ViT base model as pretrained model\n",
    "model = TFViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10c5f539-308e-480d-bd07-b3fecb3eaf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 9s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
       "array([844, 844, 844, 710, 844, 750, 761, 844, 767, 591, 767, 411, 710,\n",
       "       844, 626, 844, 767, 767, 844, 710])>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of directories with data to use in training, validation and testing\n",
    "datadir_list = [\n",
    "    '/home/felbus/transformers/data/Lego', \n",
    "    '/home/felbus/transformers/data/Duplo'\n",
    "]\n",
    "\n",
    "# Definition of useful classes and functions for later Papeline creation\n",
    "\n",
    "class ParseCat:\n",
    "    '''Class helps to transform string labels to integer numbers. Therefore an initially\n",
    "    empty dictionary is filled with an pair (categry_str: #), when class is called. \n",
    "    # is the integer representation of a category string value and is generated automatically,\n",
    "    when class is called with an unknown category string value. Class returns # uppon call.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.cat_dict = {} # emtpy dictionary to hold ('category': #) pairs\n",
    "\n",
    "    def __call__(self, category):\n",
    "        # Dynamically define #, if category is unkwown, return #\n",
    "        if (cat_int := self.cat_dict.get(category)) is None: \n",
    "            self.cat_dict[category] = cat_int = len(self.cat_dict)\n",
    "        return cat_int # return #\n",
    "\n",
    "    def __str__(self):\n",
    "        # Generate human readable output for print()\n",
    "        content_str = f'#\\t\\tCategory\\n-------------------------------------\\n'\n",
    "        for i, (key, value) in enumerate(self.cat_dict.items()):\n",
    "            content_str += f'{value}\\t\\t{key}\\n'\n",
    "        return content_str\n",
    "        \n",
    "\n",
    "@tf.py_function(Tout=tf.float32)\n",
    "def apply_feature_extractor(x):\n",
    "    '''Function will apply \"feature_extractor\" to image data. Decorator allows usage of this function inside tf.data.Dataset.map().\n",
    "    Parameters:\n",
    "    x <tf.Tensor>: image data\n",
    "\n",
    "    Return:\n",
    "    <tf.Tensor>: features of image data with shape (channels, heigth, width)\n",
    "    '''\n",
    "    return feature_extractor.preprocess(images=x, return_tensors='tf')['pixel_values']\n",
    "\n",
    "lego_parse = ParseCat()\n",
    "\n",
    "def list_file_generator(dir_list):\n",
    "    '''Generator function will take a list with directory paths and yield tuples\n",
    "    of two strings (filepath, label). Label will be extracted from lowest part of\n",
    "    directory path and is pared to an integer category number.\n",
    "    Parameters:\n",
    "    dir_list <list <str>>: list of strings containing paths to directories\n",
    "\n",
    "    Return:\n",
    "    <tuple (<str>, <int>): Tuple containing absolute file path and label category\n",
    "    '''\n",
    "    # Loop over all directories passed in dir_list\n",
    "    for directory in dir_list:\n",
    "        # Decode bytestring to string\n",
    "        directory = directory.decode('utf-8')\n",
    "        # Take lowest directory name as label and parse it to integer\n",
    "        _, label_str = os.path.split(directory)\n",
    "        label = lego_parse(label_str)\n",
    "        # Create list of all files in directory and loop over these files\n",
    "        file_list = os.listdir(directory)\n",
    "        for file in file_list:\n",
    "            # join path and filename to absolute file path of this file\n",
    "            file_path = os.path.join(directory, file)\n",
    "            # yield tupel (absolute file path to image file, label)\n",
    "            yield file_path, label\n",
    "            \n",
    "def load_image_data(path):\n",
    "    '''Function will load image data and process it with tensorflow decode_image function.\n",
    "    Parameters:\n",
    "    path <str>: file path string to load the image from\n",
    "\n",
    "    Return:\n",
    "    <tf.Tensor>: Decoded image with shape (height, width, channels)\n",
    "    '''\n",
    "    image = tf.io.read_file(path)\n",
    "    return tf.io.decode_image(image, channels=3)\n",
    "\n",
    "# Creation of Pipeline object tf.data.Dataset\n",
    "\n",
    "# Initiate an tf.data.Dataset instance from generator function. \n",
    "# Elements will be of type tuple with two Tensors containing a string (file path and label)\n",
    "image_dataset = tf.data.Dataset.from_generator(\n",
    "    list_file_generator, args=[datadir_list], output_signature=(\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string), # file path\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int16) # label category\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Load image data by mapping file path to load_image_data function\n",
    "image_dataset = image_dataset.map(lambda x, y: (load_image_data(x), y))\n",
    "\n",
    "\n",
    "# Wrap call of ViTImageProcessor in batch() / unbatch(), because otherwise it will add a dimension, that is not\n",
    "# recognized as a batching dimension by the tf.data.Dataset instance.\n",
    "image_dataset = image_dataset.batch(2)\n",
    "# Apply feature_extractor to all image data, data in pipeline will be \"channel dimension first\"\n",
    "# Images will be rescaled to [0, 1] and then normalized to means [0.5, 0.5, 0.5], resized to 3x224x224\n",
    "image_dataset = image_dataset.map(lambda x, y: (apply_feature_extractor(x), y))\n",
    "image_dataset = image_dataset.unbatch()\n",
    "\n",
    "# Create new dataset from iterator over image_dataset. This is a work around, as the\n",
    "# return of ViTImageProcessor somehow returns a Dataset, that is unsuitable to be \n",
    "# passed to TFViTForImageClassification.\n",
    "image_dataset = tf.data.Dataset.from_generator(\n",
    "    image_dataset.__iter__, output_signature=(\n",
    "        tf.TensorSpec(shape=(3, 224, 224), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int16)\n",
    "    )\n",
    ")\n",
    "# Batching \n",
    "image_dataset = image_dataset.batch(3)\n",
    "\n",
    "tf.math.argmax(model.predict(image_dataset).logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36860a79-0560-4479-9437-51ae7e7a3ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\t\tCategory\n",
      "-------------------------------------\n",
      "0\t\tLego\n",
      "1\t\tDuplo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(lego_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ceaaed58-c2d4-4d0d-8b94-216a29f889c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "b'~/transformers/data/Lego'\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "b'~/transformers/data/Duplo'\n"
     ]
    }
   ],
   "source": [
    "def list_files_from_directory(path):\n",
    "    print(type(path))\n",
    "    return path\n",
    "\n",
    "data_folders = ['~/transformers/data/Lego', '~/transformers/data/Duplo']\n",
    "# Create Dataset structure over list of datafolders and use tf.data.Dataset.interleave to create Dataset of image data\n",
    "image_pipeline = tf.data.Dataset.from_tensor_slices(data_folders)\n",
    "image_pipeline = image_pipeline.map(lambda x: list_files_from_directory(x))\n",
    "#image_pipeline = image_pipeline.interleave(lambda x: list_files_from_directory(x))\n",
    "for i, val in enumerate(image_pipeline):\n",
    "    print(type(val))\n",
    "    print(val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25018d33-b179-4dc1-aa59-de1b0c5b7f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int64, numpy=array([285, 285, 282])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(image_pipeline)\n",
    "tf.argmax(prediction.logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "962f2832-e451-4bc1-8d32-02414604a520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create tf.data.Dataset containing all \".jpg\" file names in folder\n",
    "test = tf.data.Dataset.list_files('*.jpg')\n",
    "# Load the image date from file names\n",
    "test = test.map(lambda x: load_image_data(x))\n",
    "\n",
    "for i, val in enumerate(test):\n",
    "    print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "921d7aa0-3fc0-4ba0-9173-1798e9b1d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor's shape (3, 224, 224) is not compatible with supplied shape [224, 224, 3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_pipeline\u001b[38;5;241m.\u001b[39munbatch()):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(val[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#plt.imshow(val[0])\u001b[39;00m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:534\u001b[0m, in \u001b[0;36m_EagerTensorBase.set_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, shape) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    533\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mis_compatible_with(shape):\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith supplied shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor's shape (3, 224, 224) is not compatible with supplied shape [224, 224, 3]."
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(image_pipeline.unbatch()):\n",
    "    print(val[0].numpy().shape)\n",
    "    print(val[0].set_shape([224, 224, 3]).shape)\n",
    "    #plt.imshow(val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4b7557a-c142-4410-930d-54f6bf1cc2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.data.ops.options.Options at 0x7f5106db8280>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ToDo : Learn about shapes and that shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29349788-e1f2-45cb-9b11-de660b9df92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 492ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic = tf.io.read_file('sample.jpg')\n",
    "pic = tf.io.decode_image(pic)\n",
    "pic_features = feature_extractor(pic, return_tensors='tf')\n",
    "prediction = model.predict(pic_features['pixel_values'])\n",
    "pic_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7219e909-ce28-4458-a768-d98190f46502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
