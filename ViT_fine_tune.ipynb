{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73771b80-f5ad-4514-b451-62f744d0f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks\n",
    "from transformers import ViTImageProcessor, TFViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d27b526-0c84-4add-a3c8-17d695d8f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_vi_t_for_image_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  769000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86567656 (330.23 MB)\n",
      "Trainable params: 86567656 (330.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ViTImageProcessor and use ViT base model as pretrained model\n",
    "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "# Create an instance of ViTForImageClassification and use ViT base model as pretrained model\n",
    "model = TFViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29457a2f-147e-4a21-bc3a-06ba7baa3c62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Function will apply \"feature_extractor\" to image data. Decorator allows usage of this function inside tf.data.Dataset.map().\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    x <tf.Tensor>: image data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    <tf.Tensor>: features of image data with shape (channels, heigth, width)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feature_extractor\u001b[38;5;241m.\u001b[39mpreprocess(images\u001b[38;5;241m=\u001b[39mx, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m)))\n\u001b[1;32m     24\u001b[0m sub_folders \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/Lego\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/Duplo\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m abs_folders \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "def load_image_data(path):\n",
    "    '''Function will load image data and process it with tensorflow decode_image function.\n",
    "    Parameters:\n",
    "    path <str>: file path string to load the image from\n",
    "\n",
    "    Return:\n",
    "    <tf.Tensor>: Decoded image with shape (height, width, channels)\n",
    "    '''\n",
    "    image = tf.io.read_file(path)\n",
    "    return tf.io.decode_image(image, channels=3)\n",
    "\n",
    "@tf.py_function(Tout=tf.float32)\n",
    "def apply_feature_extractor(x):\n",
    "    '''Function will apply \"feature_extractor\" to image data. Decorator allows usage of this function inside tf.data.Dataset.map().\n",
    "    Parameters:\n",
    "    x <tf.Tensor>: image data\n",
    "\n",
    "    Return:\n",
    "    <tf.Tensor>: features of image data with shape (channels, heigth, width)\n",
    "    '''\n",
    "    return feature_extractor.preprocess(images=x, return_tensors='tf')['pixel_values']\n",
    "\n",
    "os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "sub_folders = ['/data/Lego', '/data/Duplo']\n",
    "abs_folders = []\n",
    "\n",
    "for i, val in enumerate(sub_folders):\n",
    "    os.chdir('..')\n",
    "    print(os.getcwd())\n",
    "    os.chdir(val)\n",
    "    abs_folders.append(os.getcwd())\n",
    "## change back to \n",
    "\n",
    "\n",
    "# Create tf.data.Dataset containing all \".jpg\" file names in folder\n",
    "image_pipeline = tf.data.Dataset.list_files('*.jpg')\n",
    "# Adding labels \n",
    "image_pipeline = image_pipeline.map(lambda x: (x, 'cat'))\n",
    "# Load the image date from file names\n",
    "image_pipeline = image_pipeline.map(lambda x, y: (load_image_data(x), y))\n",
    "\n",
    "# Wrap call of ViTImageProcessor in batch() / unbatch(), because otherwise it will add a dimension, that is not\n",
    "# recognized as a batching dimension by the tf.data.Dataset instance.\n",
    "image_pipeline = image_pipeline.batch(2)\n",
    "# Apply feature_extractor to all image data, data in pipeline will be \"channel dimension first\"\n",
    "image_pipeline = image_pipeline.map(lambda x, y: (apply_feature_extractor(x), y))\n",
    "image_pipeline = image_pipeline.unbatch()\n",
    "\n",
    "\n",
    "for i, val in enumerate(image_pipeline):\n",
    "    print(tf.TensorSpec.from_tensor(val[1]))\n",
    "image_pipeline = tf.data.Dataset.from_generator(\n",
    "    image_pipeline.__iter__, output_signature=(\n",
    "        tf.TensorSpec(shape=(3, 224, 224), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string)\n",
    "    )\n",
    ")\n",
    "# Batching \n",
    "image_pipeline = image_pipeline.batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25018d33-b179-4dc1-aa59-de1b0c5b7f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int64, numpy=array([285, 285, 282])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(image_pipeline)\n",
    "tf.argmax(prediction.logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "962f2832-e451-4bc1-8d32-02414604a520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create tf.data.Dataset containing all \".jpg\" file names in folder\n",
    "test = tf.data.Dataset.list_files('*.jpg')\n",
    "# Load the image date from file names\n",
    "test = test.map(lambda x: load_image_data(x))\n",
    "\n",
    "for i, val in enumerate(test):\n",
    "    print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "921d7aa0-3fc0-4ba0-9173-1798e9b1d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor's shape (3, 224, 224) is not compatible with supplied shape [224, 224, 3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_pipeline\u001b[38;5;241m.\u001b[39munbatch()):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(val[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#plt.imshow(val[0])\u001b[39;00m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:534\u001b[0m, in \u001b[0;36m_EagerTensorBase.set_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, shape) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    533\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mis_compatible_with(shape):\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith supplied shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor's shape (3, 224, 224) is not compatible with supplied shape [224, 224, 3]."
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(image_pipeline.unbatch()):\n",
    "    print(val[0].numpy().shape)\n",
    "    print(val[0].set_shape([224, 224, 3]).shape)\n",
    "    #plt.imshow(val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4b7557a-c142-4410-930d-54f6bf1cc2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.data.ops.options.Options at 0x7f5106db8280>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ToDo : Learn about shapes and that shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29349788-e1f2-45cb-9b11-de660b9df92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 492ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic = tf.io.read_file('sample.jpg')\n",
    "pic = tf.io.decode_image(pic)\n",
    "pic_features = feature_extractor(pic, return_tensors='tf')\n",
    "prediction = model.predict(pic_features['pixel_values'])\n",
    "pic_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7219e909-ce28-4458-a768-d98190f46502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
