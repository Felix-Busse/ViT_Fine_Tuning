{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73771b80-f5ad-4514-b451-62f744d0f62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 12:56:03.005412: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-11 12:56:03.359976: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-11 12:56:03.360044: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-11 12:56:03.368019: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-11 12:56:03.396883: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-11 12:56:03.398858: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 12:56:04.892953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import imageio.v2 as iio\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks\n",
    "from transformers import ViTImageProcessor, TFViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d27b526-0c84-4add-a3c8-17d695d8f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_vi_t_for_image_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  769000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86567656 (330.23 MB)\n",
      "Trainable params: 86567656 (330.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ViTImageProcessor and use ViT base model as pretrained model\n",
    "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "# Create an instance of ViTForImageClassification and use ViT base model as pretrained model\n",
    "model = TFViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29457a2f-147e-4a21-bc3a-06ba7baa3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_data(path):\n",
    "    '''Function will load image data and process it with tensorflow decode_image function.\n",
    "    Parameters:\n",
    "    path <str>: file path string to load the image from\n",
    "\n",
    "    Return:\n",
    "    <tf.Tensor>: Decoded image with shape (height, width, channels)\n",
    "    '''\n",
    "    image = tf.io.read_file(path)\n",
    "    return tf.io.decode_image(image)\n",
    "\n",
    "@tf.py_function(Tout=tf.float32)\n",
    "def apply_feature_extractor(x):\n",
    "    '''Function will apply \"feature_extractor\" to image data. Decorator allows usage of this function inside tf.data.Dataset.map().\n",
    "    Parameters:\n",
    "    x <tf.Tensor>: image data\n",
    "\n",
    "    Return:\n",
    "    <tf.Tensor>: features of image data\n",
    "    '''\n",
    "    return feature_extractor(images=x, return_tensors='tf')['pixel_values']\n",
    "\n",
    "# Create tf.data.Dataset containing all \".jpg\" file names in folder\n",
    "image_pipeline = tf.data.Dataset.list_files('*.jpg')\n",
    "# Load the image date from file names\n",
    "image_pipeline = image_pipeline.map(lambda x: load_image_data(x))\n",
    "# Batch Dataset\n",
    "image_pipeline = image_pipeline.batch(1)\n",
    "# Apply feature_extractor to all image data\n",
    "image_pipeline = image_pipeline.map(lambda x: apply_feature_extractor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7b174be-2884-48c3-9e26-5bee0b3d81a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[-0.8745098  -0.8901961  -0.92156863 ... -0.4980392  -0.4980392\n",
      "    -0.4980392 ]\n",
      "   [-0.8509804  -0.8901961  -0.90588236 ... -0.4980392  -0.4980392\n",
      "    -0.4980392 ]\n",
      "   [-0.90588236 -0.9529412  -0.9529412  ... -0.4980392  -0.4980392\n",
      "    -0.4980392 ]\n",
      "   ...\n",
      "   [-0.372549   -0.38039213 -0.3960784  ... -0.4980392  -0.5058824\n",
      "    -0.5137255 ]\n",
      "   [-0.40392154 -0.41176468 -0.41960782 ... -0.5137255  -0.5294118\n",
      "    -0.5372549 ]\n",
      "   [-0.41960782 -0.41960782 -0.42745095 ... -0.5058824  -0.52156866\n",
      "    -0.5372549 ]]\n",
      "\n",
      "  [[-0.96862745 -0.9843137  -1.         ... -0.47450978 -0.47450978\n",
      "    -0.47450978]\n",
      "   [-0.9529412  -0.9764706  -0.9843137  ... -0.47450978 -0.47450978\n",
      "    -0.47450978]\n",
      "   [-0.99215686 -1.         -1.         ... -0.47450978 -0.47450978\n",
      "    -0.47450978]\n",
      "   ...\n",
      "   [-0.41176468 -0.41960782 -0.4352941  ... -0.5372549  -0.5529412\n",
      "    -0.5529412 ]\n",
      "   [-0.4588235  -0.46666664 -0.47450978 ... -0.56078434 -0.5764706\n",
      "    -0.58431375]\n",
      "   [-0.47450978 -0.47450978 -0.4823529  ... -0.5529412  -0.5686275\n",
      "    -0.58431375]]\n",
      "\n",
      "  [[-0.9372549  -0.96862745 -1.         ... -0.4352941  -0.4352941\n",
      "    -0.4352941 ]\n",
      "   [-0.8980392  -0.94509804 -0.99215686 ... -0.4352941  -0.4352941\n",
      "    -0.4352941 ]\n",
      "   [-0.92941177 -0.9843137  -1.         ... -0.4352941  -0.4352941\n",
      "    -0.4352941 ]\n",
      "   ...\n",
      "   [-0.44313723 -0.45098037 -0.46666664 ... -0.56078434 -0.5529412\n",
      "    -0.5764706 ]\n",
      "   [-0.5058824  -0.5137255  -0.52156866 ... -0.56078434 -0.56078434\n",
      "    -0.58431375]\n",
      "   [-0.52156866 -0.52156866 -0.5294118  ... -0.5372549  -0.5529412\n",
      "    -0.5686275 ]]]], shape=(1, 3, 224, 224), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-0.75686276 -0.7490196  -0.7490196  ... -0.56078434 -0.5529412\n",
      "    -0.56078434]\n",
      "   [-0.75686276 -0.7490196  -0.7490196  ... -0.56078434 -0.5529412\n",
      "    -0.56078434]\n",
      "   [-0.75686276 -0.7490196  -0.7490196  ... -0.56078434 -0.56078434\n",
      "    -0.5686275 ]\n",
      "   ...\n",
      "   [ 0.05882359  0.01176476 -0.03529412 ...  0.11372554  0.07450986\n",
      "     0.02745104]\n",
      "   [ 0.10588241  0.05882359  0.01176476 ...  0.12941182  0.082353\n",
      "     0.03529418]\n",
      "   [ 0.13725495  0.09803927  0.05098045 ...  0.13725495  0.082353\n",
      "     0.03529418]]\n",
      "\n",
      "  [[-0.75686276 -0.7490196  -0.7490196  ... -0.5372549  -0.54509807\n",
      "    -0.5529412 ]\n",
      "   [-0.75686276 -0.7490196  -0.7490196  ... -0.5372549  -0.54509807\n",
      "    -0.5529412 ]\n",
      "   [-0.75686276 -0.7490196  -0.7490196  ... -0.5372549  -0.5529412\n",
      "    -0.56078434]\n",
      "   ...\n",
      "   [ 0.17647064  0.12941182  0.082353   ...  0.0196079  -0.02745098\n",
      "    -0.0745098 ]\n",
      "   [ 0.2313726   0.18431377  0.13725495 ...  0.03529418 -0.01960784\n",
      "    -0.06666666]\n",
      "   [ 0.26274514  0.22352946  0.17647064 ...  0.04313731 -0.01960784\n",
      "    -0.06666666]]\n",
      "\n",
      "  [[-0.75686276 -0.7490196  -0.7490196  ... -0.4980392  -0.5058824\n",
      "    -0.5137255 ]\n",
      "   [-0.75686276 -0.7490196  -0.7490196  ... -0.4980392  -0.5058824\n",
      "    -0.5137255 ]\n",
      "   [-0.75686276 -0.7490196  -0.7490196  ... -0.4980392  -0.5137255\n",
      "    -0.52156866]\n",
      "   ...\n",
      "   [ 0.15294123  0.10588241  0.05882359 ... -0.10588235 -0.15294117\n",
      "    -0.19999999]\n",
      "   [ 0.20784318  0.16078436  0.11372554 ... -0.09019607 -0.14509803\n",
      "    -0.19215685]\n",
      "   [ 0.23921573  0.20000005  0.15294123 ... -0.08235294 -0.14509803\n",
      "    -0.19215685]]]], shape=(1, 3, 224, 224), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[ 0.11372554  0.12156868  0.15294123 ... -0.4588235  -0.4588235\n",
      "    -0.4588235 ]\n",
      "   [ 0.06666672  0.10588241  0.17647064 ... -0.41960782 -0.42745095\n",
      "    -0.42745095]\n",
      "   [ 0.07450986  0.12941182  0.21568632 ... -0.38039213 -0.38823527\n",
      "    -0.3960784 ]\n",
      "   ...\n",
      "   [-0.6        -0.5137255  -0.38039213 ...  0.23921573  0.12941182\n",
      "     0.09803927]\n",
      "   [-0.5137255  -0.44313723 -0.3333333  ...  0.2941177   0.15294123\n",
      "     0.05882359]\n",
      "   [-0.46666664 -0.41960782 -0.34117645 ...  0.27843142  0.1686275\n",
      "     0.05882359]]\n",
      "\n",
      "  [[-0.04313725 -0.03529412 -0.01960784 ... -0.45098037 -0.45098037\n",
      "    -0.45098037]\n",
      "   [-0.09019607 -0.04313725  0.00392163 ... -0.41176468 -0.41960782\n",
      "    -0.41960782]\n",
      "   [-0.0745098  -0.01960784  0.04313731 ... -0.372549   -0.38039213\n",
      "    -0.38823527]\n",
      "   ...\n",
      "   [-0.64705884 -0.56078434 -0.42745095 ...  0.16078436  0.05098045\n",
      "     0.0196079 ]\n",
      "   [-0.5686275  -0.4980392  -0.38823527 ...  0.19215691  0.07450986\n",
      "    -0.01960784]\n",
      "   [-0.52156866 -0.47450978 -0.3960784  ...  0.17647064  0.09019613\n",
      "    -0.01960784]]\n",
      "\n",
      "  [[-0.3333333  -0.32549018 -0.32549018 ... -0.4980392  -0.4980392\n",
      "    -0.4980392 ]\n",
      "   [-0.38039213 -0.35686272 -0.30196077 ... -0.4588235  -0.46666664\n",
      "    -0.46666664]\n",
      "   [-0.38823527 -0.3333333  -0.26274508 ... -0.41960782 -0.42745095\n",
      "    -0.4352941 ]\n",
      "   ...\n",
      "   [-0.75686276 -0.67058825 -0.5372549  ...  0.082353   -0.02745098\n",
      "    -0.05882353]\n",
      "   [-0.69411767 -0.62352943 -0.5137255  ...  0.12156868 -0.00392157\n",
      "    -0.09803921]\n",
      "   [-0.64705884 -0.6        -0.52156866 ...  0.10588241  0.01176476\n",
      "    -0.09803921]]]], shape=(1, 3, 224, 224), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(image_pipeline):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "921d7aa0-3fc0-4ba0-9173-1798e9b1d429",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2436, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2409, in run_step  *\n        outputs = model.predict_step(data)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2377, in predict_step  *\n        return self(x, training=False)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__  *\n        return super().__call__(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 162, in error_handler  **\n        raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n    File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 34, in error_handler\n        retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification).\n    \n    in user code:\n    \n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 872, in call  *\n            outputs = self.vit(\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n            return fn(*args, **kwargs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n            outputs = call_fn(inputs, *args, **kwargs)\n        File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 162, in error_handler  **\n            raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n        File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 34, in error_handler\n            retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n    \n        OperatorNotAllowedInGraphError: Exception encountered when calling layer 'vit' (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 596, in call  *\n                embedding_output = self.embeddings(\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n                return fn(*args, **kwargs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n                outputs = call_fn(inputs, *args, **kwargs)\n            File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 162, in error_handler  **\n                raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n            File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 34, in error_handler\n                retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n        \n            OperatorNotAllowedInGraphError: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 129, in call  *\n                    batch_size, num_channels, height, width = shape_list(pixel_values)\n            \n                OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n            \n            \n            Call arguments received by layer 'embeddings' (type TFViTEmbeddings):\n              • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=False\n        \n        \n        Call arguments received by layer 'vit' (type TFViTMainLayer):\n          • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n          • head_mask=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification):\n      • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • labels=None\n      • training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_pipeline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1249\u001b[0m, in \u001b[0;36mTFPreTrainedModel.predict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mpredict)\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1248\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2436, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2409, in run_step  *\n        outputs = model.predict_step(data)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 2377, in predict_step  *\n        return self(x, training=False)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__  *\n        return super().__call__(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 162, in error_handler  **\n        raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n    File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 34, in error_handler\n        retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification).\n    \n    in user code:\n    \n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 872, in call  *\n            outputs = self.vit(\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n            return fn(*args, **kwargs)\n        File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n            outputs = call_fn(inputs, *args, **kwargs)\n        File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 162, in error_handler  **\n            raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n        File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 34, in error_handler\n            retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n    \n        OperatorNotAllowedInGraphError: Exception encountered when calling layer 'vit' (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 863, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 596, in call  *\n                embedding_output = self.embeddings(\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n                return fn(*args, **kwargs)\n            File \"/home/felbus/transformers/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n                outputs = call_fn(inputs, *args, **kwargs)\n            File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 162, in error_handler  **\n                raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n            File \"/tmp/__autograph_generated_file26rrhbe7.py\", line 34, in error_handler\n                retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n        \n            OperatorNotAllowedInGraphError: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"/home/felbus/transformers/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 129, in call  *\n                    batch_size, num_channels, height, width = shape_list(pixel_values)\n            \n                OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n            \n            \n            Call arguments received by layer 'embeddings' (type TFViTEmbeddings):\n              • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=False\n        \n        \n        Call arguments received by layer 'vit' (type TFViTMainLayer):\n          • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n          • head_mask=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification):\n      • pixel_values=tf.Tensor(shape=<unknown>, dtype=float32)\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • labels=None\n      • training=False\n"
     ]
    }
   ],
   "source": [
    "model.predict(image_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29349788-e1f2-45cb-9b11-de660b9df92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 431ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 224, 224), dtype=float32, numpy=\n",
       "array([[[[ 0.11372554,  0.12156868,  0.15294123, ..., -0.4588235 ,\n",
       "          -0.4588235 , -0.4588235 ],\n",
       "         [ 0.06666672,  0.10588241,  0.17647064, ..., -0.41960782,\n",
       "          -0.42745095, -0.42745095],\n",
       "         [ 0.07450986,  0.12941182,  0.21568632, ..., -0.38039213,\n",
       "          -0.38823527, -0.3960784 ],\n",
       "         ...,\n",
       "         [-0.6       , -0.5137255 , -0.38039213, ...,  0.23921573,\n",
       "           0.12941182,  0.09803927],\n",
       "         [-0.5137255 , -0.44313723, -0.3333333 , ...,  0.2941177 ,\n",
       "           0.15294123,  0.05882359],\n",
       "         [-0.46666664, -0.41960782, -0.34117645, ...,  0.27843142,\n",
       "           0.1686275 ,  0.05882359]],\n",
       "\n",
       "        [[-0.04313725, -0.03529412, -0.01960784, ..., -0.45098037,\n",
       "          -0.45098037, -0.45098037],\n",
       "         [-0.09019607, -0.04313725,  0.00392163, ..., -0.41176468,\n",
       "          -0.41960782, -0.41960782],\n",
       "         [-0.0745098 , -0.01960784,  0.04313731, ..., -0.372549  ,\n",
       "          -0.38039213, -0.38823527],\n",
       "         ...,\n",
       "         [-0.64705884, -0.56078434, -0.42745095, ...,  0.16078436,\n",
       "           0.05098045,  0.0196079 ],\n",
       "         [-0.5686275 , -0.4980392 , -0.38823527, ...,  0.19215691,\n",
       "           0.07450986, -0.01960784],\n",
       "         [-0.52156866, -0.47450978, -0.3960784 , ...,  0.17647064,\n",
       "           0.09019613, -0.01960784]],\n",
       "\n",
       "        [[-0.3333333 , -0.32549018, -0.32549018, ..., -0.4980392 ,\n",
       "          -0.4980392 , -0.4980392 ],\n",
       "         [-0.38039213, -0.35686272, -0.30196077, ..., -0.4588235 ,\n",
       "          -0.46666664, -0.46666664],\n",
       "         [-0.38823527, -0.3333333 , -0.26274508, ..., -0.41960782,\n",
       "          -0.42745095, -0.4352941 ],\n",
       "         ...,\n",
       "         [-0.75686276, -0.67058825, -0.5372549 , ...,  0.082353  ,\n",
       "          -0.02745098, -0.05882353],\n",
       "         [-0.69411767, -0.62352943, -0.5137255 , ...,  0.12156868,\n",
       "          -0.00392157, -0.09803921],\n",
       "         [-0.64705884, -0.6       , -0.52156866, ...,  0.10588241,\n",
       "           0.01176476, -0.09803921]]]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic = tf.io.read_file('sample.jpg')\n",
    "pic = tf.io.decode_image(pic)\n",
    "pic_features = feature_extractor(pic, return_tensors='tf')\n",
    "prediction = model.predict(pic_features['pixel_values'])\n",
    "pic_features['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7219e909-ce28-4458-a768-d98190f46502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m category_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39margmax(prediction\u001b[38;5;241m.\u001b[39mlogits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/keras/src/activations.py:87\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.activations.softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Softmax converts a vector of values to a probability distribution.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    The elements of the output vector are in range (0, 1) and sum to 1.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    ...                               activation=tf.keras.activations.softmax)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers/lib/python3.10/site-packages/keras/src/backend.py:5442\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m   5428\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.backend.softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5429\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   5430\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_generate_docs\n\u001b[1;32m   5431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   5432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Softmax of a tensor.\u001b[39;00m\n\u001b[1;32m   5433\u001b[0m \n\u001b[1;32m   5434\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5440\u001b[0m \u001b[38;5;124;03m        A tensor.\u001b[39;00m\n\u001b[1;32m   5441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   5443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5444\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot apply softmax to a tensor that is 1D. Received input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5445\u001b[0m         )\n\u001b[1;32m   5447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(axis, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
     ]
    }
   ],
   "source": [
    "category_tensor = tf.math.argmax(prediction.logits, axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
